{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/DAlkemade/bert-for-fever/blob/master/L101_tokenize_for_sentence_selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idBlfvt45Tga",
    "colab_type": "text"
   },
   "source": [
    "Tokenize the data in a .tsv file of features to use as input for a BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "HJjjbEf-2tMd",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install transformers\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "p8WRYYbj14bE",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "import torch\n",
    "from transformers import *\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset, WeightedRandomSampler)\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "7YWkleQ01ZR-",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "TEST = False\n",
    "MAX_SENTENCE_LENGTH = 256 #maybe make smaller and then batch size higher\n",
    "PADDING_TOKEN_TYPE_ID = 0 # take the advice at https://github.com/huggingface/transformers/blob/0cb163865a4c761c226b151283309eedb2b1ca4d/transformers/data/processors/glue.py#L30\n",
    "WORK_DIR = '/content/drive/My Drive/Overig'\n",
    "ADD_TITLE_TO_SENTENCE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "aaeKUeh35SXA",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')\n",
    "data_fname = '/content/drive/My Drive/Overig/dev_sentences_from_bert_doc_selector.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "uuha10L2CHCC",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "pS35k9Jn1zVw",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_fname)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "U_UsPD5Zb6qB",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    data= data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "isjTxH0W17pa",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "print(f\"Number of training instances: {len(data.index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "NxmYz4t96nxw",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "pad_token = tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "8CeQBeC1ipxh",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def prep_instance(claim, source_sentence, label, doc_id, step):\n",
    "    if step % 50000 == 0:\n",
    "        print(f'At step {step}')\n",
    "    \n",
    "    # print(f'Total length: {len(claim) + len(source_sentence)}')\n",
    "    if ADD_TITLE_TO_SENTENCE:\n",
    "        source_sentence = f'[ {doc_id} ] {source_sentence}'\n",
    "    encodings = tokenizer.encode_plus(claim, source_sentence, add_special_tokens=True, max_length=MAX_SENTENCE_LENGTH)\n",
    "    input_ids, token_type_ids = encodings[\"input_ids\"], encodings[\"token_type_ids\"]\n",
    "    # We mask padding with 0\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    # Pad on the right\n",
    "    padding_length = MAX_SENTENCE_LENGTH - len(input_ids)\n",
    "    # The next 3 lines are taken from the example at https://github.com/huggingface/transformers/blob/0cb163865a4c761c226b151283309eedb2b1ca4d/transformers/data/processors/glue.py#L30\n",
    "    input_ids = input_ids + ([pad_token] * padding_length)\n",
    "    # We mask padding with 0\n",
    "    attention_mask = attention_mask + ([0] * padding_length)\n",
    "    token_type_ids = token_type_ids + ([PADDING_TOKEN_TYPE_ID] * padding_length)\n",
    "    return InputFeatures(input_ids=input_ids,attention_mask=attention_mask,\n",
    "                              token_type_ids=token_type_ids, label=label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "YTRFKwrqPjFi",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def create_features(data, dev=False):\n",
    "    claims = list(data.claim)\n",
    "    sentences_source = list(data.sentence)\n",
    "    labels = list(data.evidence)\n",
    "    ids = list(data.id)\n",
    "    doc_ids = list(data.doc_id)\n",
    "    features = [prep_instance(claims[i], sentences_source[i], labels[i], doc_ids[i], i) for i in range(len(claims))]\n",
    "    print(\"Save features\")\n",
    "    torch.save(features, os.path.join(WORK_DIR, f'{datetime.now().strftime(\"%y%m%d%H%M%S\")}features_include_title={ADD_TITLE_TO_SENTENCE}_from_{data_fname.split(\".\")[0].split(\"/\")[-1]}'))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "UZi6W7v47ORc",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "print(\"Create features\")\n",
    "features = create_features(data)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "L101 tokenize for sentence selection",
   "provenance": [],
   "collapsed_sections": [],
   "machine_shape": "hm",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
