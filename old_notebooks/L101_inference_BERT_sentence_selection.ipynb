{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/DAlkemade/bert-for-fever/blob/master/L101_inference_BERT_sentence_selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWe3v7PX-8Te",
    "colab_type": "text"
   },
   "source": [
    "TODO:\n",
    "- Loop over dev.sentences.p5.s5.jsonl om alle zinnen uit de predicted_documents  te halen (eventueel in los notebook (tsv maken))\n",
    "- Maak een feautures object met tokenize notebook\n",
    "- Maak in dit notebook een lijst met ids van de .tsv en laadt bijbehorende features object. Zorg dat id weer in de TensorDataSet komt\n",
    "- Doe predictions voor alle instances in de features en sla de evidence logit op in een dict op met de vorm {'id': ['sentence': 'x', 'chance':chance of being evidence]}}\n",
    "- Loop over dev.sentences.p5.s5.jsonl en kies voor elke claim de top 5 sentences (of minder als er minder als evidence zijn geclassified, dus stel 3 hebben argmax=evidence, doe er dan 3 (betere precision)). Voor not verifiable entries gewoon een lege list oid erin stoppen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "HJjjbEf-2tMd",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install transformers\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "p8WRYYbj14bE",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "import torch\n",
    "from transformers import *\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset, WeightedRandomSampler)\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import json\n",
    "import pprint\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "7YWkleQ01ZR-",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "WORK_DIR = '/content/drive/My Drive/Overig'\n",
    "features_fname = '200110134032features_include_title=False_from_dev_sentences_from_bert_doc_selector'\n",
    "cached_features_file_dev = os.path.join(WORK_DIR, features_fname) # nog maken\n",
    "data_fname = '/content/drive/My Drive/Overig/dev_sentences_from_bert_doc_selector.tsv' # nog maken\n",
    "model_fname = 'results2ndmodel'\n",
    "N = 5\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "TEST_IDS = [137334, 145446]\n",
    "OUT_TAG = 'dev_sentences_on_bert_doc_inputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "aaeKUeh35SXA",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "uuha10L2CHCC",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "RXvf4dtOdTDv",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_fname)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "UZi6W7v47ORc",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "print(\"Load cached dev features\")\n",
    "features_dev = torch.load(cached_features_file_dev)\n",
    "print(\"Loaded features\")\n",
    "print(f'Len features: {len(features_dev)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "RlwdtpgDy26g",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "claim_ids = list(data.id)\n",
    "doc_ids = list(data.doc_id)\n",
    "sentence_idxs = list(data.sentence_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "eAXcfdu-Py1X",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def create_dataloader(features, dev=False):\n",
    "    # The next lines are taken from the example at https://github.com/huggingface/transformers/blob/0cb163865a4c761c226b151283309eedb2b1ca4d/transformers/data/processors/glue.py#L30\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
    "    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
    "    all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
    "    all_claim_ids = torch.tensor(claim_ids, dtype=torch.long)\n",
    "    idx = torch.tensor(range(len(features)), dtype=torch.long)\n",
    "\n",
    "    \n",
    "    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels, all_claim_ids, idx)\n",
    "    \n",
    "    if dev:\n",
    "        dataloader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "    else:\n",
    "        class_counts = np.bincount(all_labels)\n",
    "        class_sample_freq = 1/class_counts\n",
    "        weights = [class_sample_freq[label] for label in all_labels]\n",
    "        print(class_counts)\n",
    "        print(class_sample_freq)\n",
    "        # sampler = RandomSampler(dataset)\n",
    "        num_samples = round(class_counts[1]*2).item() #.item to convert to native int\n",
    "        print(f'Num samples: {num_samples}')\n",
    "        sampler = WeightedRandomSampler(weights, num_samples=num_samples, replacement=True) # we want to use all positive instances and use equally as many negative instances. This should now generally happen by chance\n",
    "        dataloader = DataLoader(dataset, sampler=sampler, batch_size=BATCH_SIZE)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "wWv15ZWi-NB1",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(\"Create dev dataloader\")\n",
    "dataloader_dev = create_dataloader(features_dev, dev=True)\n",
    "del features_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "AAHA7ufR-yz_",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(f\"/content/drive/My Drive/Cambridge/L101/{model_fname}\", num_labels=2)\n",
    "model.cuda()\n",
    "pass # suppress model.cuda output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "sIv2h3aDYrlq",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def unpack_batch(batch):\n",
    "    input_ids = batch[0]\n",
    "    attention_mask = batch[1]\n",
    "    type_ids = batch[2]\n",
    "    y_true = batch[3]\n",
    "    claim_id = batch[4]\n",
    "    doc_id = [doc_ids[idx] for idx in batch[5]]  #retrieve doc ids\n",
    "    sentence_idx = [sentence_idxs[idx] for idx in batch[5]]  #retrieve sentence_idx\n",
    "    return input_ids, attention_mask, type_ids, y_true, claim_id, doc_id, sentence_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Jn9NJUTnejTD",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "evidence = dict((el,[]) for el in dict.fromkeys(claim_ids))\n",
    "evidence_all_scores = dict((el,[]) for el in dict.fromkeys(claim_ids))\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "08esE86bAISW",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "print(\"Start evaluation\")\n",
    "\n",
    "# Variables for evaluation\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "conf_matrix = np.zeros((2,2))\n",
    "\n",
    "print(f\"Number of batches: {len(dataloader_dev)}\")\n",
    "for step, batch in enumerate(dataloader_dev):\n",
    "    if step % 1000 == 0:\n",
    "            print(f'\\nAt step {step}')\n",
    "    # Move batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack values\n",
    "    input_ids, attention_mask, type_ids, y_true, claim_ids_batch, doc_ids_batch, sentence_idxs_batch = unpack_batch(batch)\n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up validation. taken from https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "        outputs = model(input_ids, token_type_ids=type_ids, attention_mask=attention_mask, labels=y_true)\n",
    "        logits = outputs[1]\n",
    "\n",
    "\n",
    "    # Move logits and labels to CPU. from https://mccormickml.com/2019/07/22/BERT-fine-tuning/, as this should free up RAM\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    y_true = y_true.to('cpu').numpy()\n",
    "    pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "    y_true_flat = y_true.flatten()\n",
    "\n",
    "    for i, claim_id in enumerate(claim_ids_batch):\n",
    "        softmax_logits = softmax(logits[i]) # !!!\n",
    "        # print(softmax_logits)\n",
    "        # print(f'For claim {claim_id}; doc_id: {doc_ids_batch[i]}; sentence idx: {sentence_idxs_batch[i]}')\n",
    "        # save all regression scores to dict\n",
    "        evidence_all_scores[claim_id.item()].append([softmax_logits[1], doc_ids_batch[i], sentence_idxs_batch[i]])\n",
    "        \n",
    "        # save just the sentences labeled as evidence to dict\n",
    "        if pred_flat[i] == 1: # only if classified as evidence\n",
    "            evidence[claim_id.item()].append([softmax_logits[1], doc_ids_batch[i], sentence_idxs_batch[i]])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "wqqSRkppbLkG",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f'/content/drive/My Drive/Overig/sentence_evidence_from{features_fname}.pkl', 'wb') as f:\n",
    "    pickle.dump(evidence, f)\n",
    "with open(f'/content/drive/My Drive/Overig/sentence_evidence_all_from{features_fname}.pkl', 'wb') as f:\n",
    "    pickle.dump(evidence_all_scores, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uSiFDg0FT83",
    "colab_type": "text"
   },
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "L101 inference BERT sentence selection",
   "provenance": [],
   "collapsed_sections": [],
   "machine_shape": "hm",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
