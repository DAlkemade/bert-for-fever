{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L101 BERT FEVER classifier training",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DAlkemade/bert-for-fever/blob/master/L101_BERT_FEVER_classifier_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9cXEkRY2E-C",
        "colab_type": "text"
      },
      "source": [
        "# Train a BERT model for evidence classification for FEVER\n",
        "This notebook trains a model for evidence classification (either sentences or documents, depending on the input features)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJjjbEf-2tMd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install gputil\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8WRYYbj14bE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import torch\n",
        "from transformers import *\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset, WeightedRandomSampler)\n",
        "from tqdm import tqdm, trange\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import os\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import random\n",
        "import GPUtil"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YWkleQ01ZR-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEST = False\n",
        "BATCH_SIZE = 10\n",
        "WORK_DIR = '/content/drive/My Drive/Overig'\n",
        "REDUCE_TRAINING_DATA = True\n",
        "cached_features_file_train = os.path.join(WORK_DIR, '200103000018features_document_selection_from_document_selection_train_n=5')\n",
        "EPOCHS = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaeKUeh35SXA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuha10L2CHCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZi6W7v47ORc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Load cached training features\")\n",
        "features_train = torch.load(cached_features_file_train)\n",
        "len(features_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSEyIBNVdQlc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.bincount(torch.tensor([f.label for f in features_train], dtype=torch.long))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lMLOfwgRqyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if REDUCE_TRAINING_DATA:\n",
        "    new_features = []\n",
        "    for f in features_train:\n",
        "        if f.label == 1:\n",
        "            new_features.append(f)\n",
        "        else:\n",
        "            #throw away 60% of neg instances at random\n",
        "            if random.uniform(0,1) < 0.5:\n",
        "                new_features.append(f)\n",
        "    features_train = new_features\n",
        "\n",
        "\n",
        "len(features_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAXcfdu-Py1X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataloader(features, dev=False):\n",
        "    # The next lines are taken from the example at https://github.com/huggingface/transformers/blob/0cb163865a4c761c226b151283309eedb2b1ca4d/transformers/data/processors/glue.py#L30\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
        "    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
        "    all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
        "    if dev:\n",
        "        all_claim_ids = torch.tensor(claim_ids, dtype=torch.long)\n",
        "        idx = torch.tensor(range(len(features)), dtype=torch.long)\n",
        "        dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels, all_claim_ids, idx)\n",
        "    else:\n",
        "        dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n",
        "    \n",
        "    if dev:\n",
        "        dataloader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
        "    else:\n",
        "        class_counts = np.bincount(all_labels)\n",
        "        class_sample_freq = 1/class_counts\n",
        "        weights = [class_sample_freq[label] for label in all_labels]\n",
        "        print(class_counts)\n",
        "        print(class_sample_freq)\n",
        "        num_samples = round(class_counts[1]*2).item() #.item to convert to native int\n",
        "        print(f'Num samples: {num_samples}')\n",
        "        sampler = WeightedRandomSampler(weights, num_samples=num_samples, replacement=True) # we want to use all positive instances and use equally as many negative instances. This should now generally happen by chance\n",
        "        dataloader = DataLoader(dataset, sampler=sampler, batch_size=BATCH_SIZE)\n",
        "    return dataloader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWv15ZWi-NB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "print(\"Create train dataloader\")\n",
        "dataloader_train = create_dataloader(features_train)\n",
        "del features_train\n",
        "print(f\"Len train dataloader: {len(dataloader_train)}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAHA7ufR-yz_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "model.cuda()\n",
        "pass # suppress model.cuda output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pauKs5qvGmEL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is a very standard piece of code for the optimizer parameters, available from many sources, e.g. https://worksheets.codalab.org/rest/bundles/0x60ed20fc419641d799f53aa0667d5713/contents/blob/basic_trainer.py\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters,\n",
        "                     lr=2e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIv2h3aDYrlq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unpack_batch(batch):\n",
        "    input_ids = batch[0]\n",
        "    attention_mask = batch[1]\n",
        "    type_ids = batch[2]\n",
        "    y_true = batch[3]\n",
        "    return input_ids, attention_mask, type_ids, y_true"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08esE86bAISW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss_set = []\n",
        "\n",
        "# use trange for a prediction of the time it will take and to record execution times\n",
        "for _ in trange(EPOCHS, desc=\"Epoch\"):\n",
        "\n",
        "    # TRAINING ON TRAIN SET\n",
        "    model.train()\n",
        "\n",
        "    # Train the data for one epoch\n",
        "    for step, batch in enumerate(dataloader_train):\n",
        "        if step % 1000 == 0:\n",
        "            print(f'\\nAt step {step}')\n",
        "        # Move batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # Unpack values\n",
        "        input_ids, attention_mask, type_ids, y_true = unpack_batch(batch)\n",
        "        # Clear out the gradients (by default they accumulate) taken from https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "        optimizer.zero_grad()\n",
        "        # Make predictions for the batch\n",
        "        outputs = model(input_ids, token_type_ids=type_ids, attention_mask=attention_mask, labels=y_true)\n",
        "        loss = outputs[0]\n",
        "        train_loss_set.append(loss.item())    \n",
        "        loss.backward() # Backpropagate\n",
        "        optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPsxVPZrBVjl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_pretrained(WORK_DIR)\n",
        "print(\"Done!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmPBWrY-ajuv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(os.path.join(WORK_DIR, \"losses.txt\"), \"w\") as f:\n",
        "    for loss in train_loss_set:\n",
        "        f.write(f'{loss}\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}