{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L101 preprocess document selection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DAlkemade/bert-for-fever/blob/master/L101_preprocess_document_selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_jWDYDSad1x",
        "colab_type": "text"
      },
      "source": [
        "# Preprocess the data\n",
        "The end result is a .tsv file with the columns:\n",
        "\n",
        "\n",
        "*   id\n",
        "*   label (evidence or not)\n",
        "*   sentence (from wikipedia)\n",
        "*   claim\n",
        "\n",
        "```bash\n",
        "jupyter notebook --NotebookApp.allow_origin='https://colab.research.google.com' --port=8888 --NotebookApp.port_retries=0\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L313qoQ77uOE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEST = False\n",
        "EMPTY_TOKEN = 'EMPTY'\n",
        "OUT_FILE_NAME = 'document_selection_test_n=50'\n",
        "LOCAL = False\n",
        "SAMPLE_NEGATIVE_INSTANCES = False\n",
        "APPEND_GOLD_DOCUMENT = False\n",
        "TEST_SET = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGiXIljhi1WE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not LOCAL:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lypDYTz8bjRr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import json\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skFsfqMrwGkO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if LOCAL:\n",
        "    fever_db = 'fever/fever.db'\n",
        "    root = 'D:/GitHubD/fever-allennlp/data'\n",
        "else:\n",
        "    fever_db = 'fever.db'\n",
        "    root = '/content/drive/My Drive/Overig/'\n",
        "\n",
        "db = os.path.join(root, fever_db)\n",
        "# in_file_fname = 'D:/GitHubD/fever-allennlp/data/dev_complete.sentences.p5.s5.jsonl'\n",
        "in_file_fname = os.path.join(root, 'test_baseline_pages.sentences.p5.s5.jsonl')\n",
        "out_file = os.path.join(root, f'{OUT_FILE_NAME}.tsv')\n",
        "\n",
        "conn = sqlite3.connect(db)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PCW5tk9dcrg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VWDypjmgv6Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_doc_text(id):\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(\n",
        "        \"SELECT lines FROM documents WHERE id = ?\",\n",
        "        (id,)\n",
        "    )\n",
        "    result = cursor.fetchone()\n",
        "    cursor.close()\n",
        "    return result\n",
        "\n",
        "def get_golden_docs(evidence):\n",
        "    all_evi = [[e[2], e[3]] for eg in instance[\"evidence\"] for e in eg if e[3] is not None] # from baseline scorer\n",
        "    docs = []\n",
        "    for entry in all_evi:\n",
        "        id = entry[0]\n",
        "        docs.append(id)\n",
        "        \n",
        "    return docs\n",
        "\n",
        "def parse_doc(doc_raw):\n",
        "    \"\"\"\n",
        "    Parse a list of lines from a raw document text, with the index in the list\n",
        "    correponding to the line index in the data entries\n",
        "    \"\"\"\n",
        "    new = []\n",
        "    lines = doc_raw.split(\"\\n\")\n",
        "    char_count = 0\n",
        "    for line in lines:\n",
        "        # print('Line: {}'.format(line))\n",
        "        line = line.split(\"\\t\")\n",
        "    #   TODO: THIS MIGHT DROP PARTS OF SENTENCES AFTER A TAB\n",
        "        if len(line) > 1 and len(line[1]) > 1:\n",
        "            new.append(line[1])\n",
        "            char_count += len(line[1])\n",
        "        else:\n",
        "            new.append(EMPTY_TOKEN)\n",
        "    chars.append(char_count)\n",
        "    return new"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T96z7W1KbbMG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: WAT DOEN WE MET DE NOT VERIFIABLES?\n",
        "with open(in_file_fname, \"r\") as in_file:\n",
        "    instances = []\n",
        "    for line in in_file:\n",
        "        instances.append(json.loads(line))\n",
        "    # print(f\"Number of instances: {len(instances)}\")\n",
        "    # instances = instances[:75000]\n",
        "   \n",
        "    training_instances = []\n",
        "    # if TEST:\n",
        "    #     new_instances = []\n",
        "    #     for ins in instances:\n",
        "    #         if ins['id'] == 18884:\n",
        "    #             new_instances.append(ins)\n",
        "    #     instances = new_instances\n",
        "    if TEST:\n",
        "        instances = instances[:100]\n",
        "    for i in tqdm(range(len(instances))):\n",
        "        instance = instances[i]\n",
        "        if TEST_SET or instance['verifiable'] != 'NOT VERIFIABLE':\n",
        "            claim = instance['claim']\n",
        "            claim_id = instance['id']\n",
        "            docs = instance['predicted_pages']\n",
        "            if APPEND_GOLD_DOCUMENT:\n",
        "                gold_docs = get_golden_docs(instance['evidence'])\n",
        "                for gold_doc in gold_docs:\n",
        "                    if gold_doc not in docs:\n",
        "                        docs.append(gold_doc) # make sure all positive examples are added to the data\n",
        "            \n",
        "            for doc_id in docs:\n",
        "                doc_raw = get_doc_text(doc_id)[0]\n",
        "\n",
        "                    \n",
        "                doc_sentences = parse_doc(doc_raw)\n",
        "                doc_as_string = ' '.join(doc_sentences)\n",
        "                doc_as_string_shortened = doc_as_string[:512]\n",
        "                context = doc_as_string\n",
        "\n",
        "                if not TEST_SET:\n",
        "                    if doc_id in gold_docs:\n",
        "                        label = 1\n",
        "                    else:\n",
        "                        label = 0\n",
        "                else:\n",
        "                    label = None                                             \n",
        "                training_instances.append([label, claim, context, claim_id, doc_id])\n",
        "    \n",
        "    \n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6pOvTBAwszu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for instance in instances:\n",
        "#     if instance['id'] == 75397:\n",
        "#         print(instance)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWxM6dcGCKuL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if SAMPLE_NEGATIVE_INSTANCES:\n",
        "    new_instances = []\n",
        "    for f in training_instances:\n",
        "        if f[0] == 1:\n",
        "            new_instances.append(f)\n",
        "        else:\n",
        "            #throw away 90% of neg instances at random\n",
        "            if random.uniform(0,1) < 0.1:\n",
        "                new_instances.append(f)\n",
        "    training_instances = new_instances\n",
        "\n",
        "\n",
        "len(training_instances)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJdvwPH1qwuB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(training_instances))\n",
        "data = pd.DataFrame(training_instances, columns =['label', 'claim', 'context', 'claim_id', 'doc_id']) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70grnlj59G3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.head(51)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQX2WqYay6Fj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.to_csv(out_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InjhhkA5Ug2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(data.index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0Or_FvW8wQv",
        "colab_type": "text"
      },
      "source": [
        "        # TODO: think about what to take as negative samples; just save all for now\n",
        "        #  But the BERT article does HNM (Hard negative Mining);\n",
        "        # look into that\n",
        "\n",
        "        #Do:\n",
        "        # get all sentences that are in the training instance. Loop over and label them using the instance data\n",
        "        # add every sentence as a line to a pandas dataframe\n",
        "        # save it as a .tsv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nBhPsydyk2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# first = pd.read_csv('/content/drive/My Drive/Overig/document_selection_train_dataset_first75000.tsv')\n",
        "# second = pd.read_csv('/content/drive/My Drive/Overig/document_selection_train_dataset_last75000.tsv')\n",
        "# combine = pd.concat([first, second])\n",
        "# combine.to_csv('/content/drive/My Drive/Overig/document_selection_train_dataset_combine.tsv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}