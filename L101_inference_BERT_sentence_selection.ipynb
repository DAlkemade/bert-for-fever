{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L101 inference BERT sentence selection",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DAlkemade/bert-for-fever/blob/master/L101_inference_BERT_sentence_selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWe3v7PX-8Te",
        "colab_type": "text"
      },
      "source": [
        "TODO:\n",
        "- Loop over dev.sentences.p5.s5.jsonl om alle zinnen uit de predicted_documents  te halen (eventueel in los notebook (tsv maken))\n",
        "- Maak een feautures object met tokenize notebook\n",
        "- Maak in dit notebook een lijst met ids van de .tsv en laadt bijbehorende features object. Zorg dat id weer in de TensorDataSet komt\n",
        "- Doe predictions voor alle instances in de features en sla de evidence logit op in een dict op met de vorm {'id': ['sentence': 'x', 'chance':chance of being evidence]}}\n",
        "- Loop over dev.sentences.p5.s5.jsonl en kies voor elke claim de top 5 sentences (of minder als er minder als evidence zijn geclassified, dus stel 3 hebben argmax=evidence, doe er dan 3 (betere precision)). Voor not verifiable entries gewoon een lege list oid erin stoppen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJjjbEf-2tMd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install torch\n",
        "!pip install transformers\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8WRYYbj14bE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import torch\n",
        "from transformers import *\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset, WeightedRandomSampler)\n",
        "from tqdm import tqdm, trange\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import os\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import json\n",
        "import pprint\n",
        "from scipy.special import softmax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YWkleQ01ZR-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "WORK_DIR = '/content/drive/My Drive/Overig'\n",
        "features_fname = '200110134032features_include_title=False_from_dev_sentences_from_bert_doc_selector'\n",
        "cached_features_file_dev = os.path.join(WORK_DIR, features_fname) # nog maken\n",
        "data_fname = '/content/drive/My Drive/Overig/dev_sentences_from_bert_doc_selector.tsv' # nog maken\n",
        "model_fname = 'results2ndmodel'\n",
        "N = 5\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "TEST_IDS = [137334, 145446]\n",
        "OUT_TAG = 'dev_sentences_on_bert_doc_inputs'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaeKUeh35SXA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuha10L2CHCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXvf4dtOdTDv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(data_fname)\n",
        "data.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZi6W7v47ORc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Load cached dev features\")\n",
        "features_dev = torch.load(cached_features_file_dev)\n",
        "print(\"Loaded features\")\n",
        "print(f'Len features: {len(features_dev)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlwdtpgDy26g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "claim_ids = list(data.id)\n",
        "doc_ids = list(data.doc_id)\n",
        "sentence_idxs = list(data.sentence_idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAXcfdu-Py1X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataloader(features, dev=False):\n",
        "    # The next lines are taken from the example at https://github.com/huggingface/transformers/blob/0cb163865a4c761c226b151283309eedb2b1ca4d/transformers/data/processors/glue.py#L30\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
        "    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
        "    all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
        "    all_claim_ids = torch.tensor(claim_ids, dtype=torch.long)\n",
        "    idx = torch.tensor(range(len(features)), dtype=torch.long)\n",
        "\n",
        "    \n",
        "    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels, all_claim_ids, idx)\n",
        "    \n",
        "    if dev:\n",
        "        dataloader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
        "    else:\n",
        "        class_counts = np.bincount(all_labels)\n",
        "        class_sample_freq = 1/class_counts\n",
        "        weights = [class_sample_freq[label] for label in all_labels]\n",
        "        print(class_counts)\n",
        "        print(class_sample_freq)\n",
        "        # sampler = RandomSampler(dataset)\n",
        "        num_samples = round(class_counts[1]*2).item() #.item to convert to native int\n",
        "        print(f'Num samples: {num_samples}')\n",
        "        sampler = WeightedRandomSampler(weights, num_samples=num_samples, replacement=True) # we want to use all positive instances and use equally as many negative instances. This should now generally happen by chance\n",
        "        dataloader = DataLoader(dataset, sampler=sampler, batch_size=BATCH_SIZE)\n",
        "    return dataloader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWv15ZWi-NB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "print(\"Create dev dataloader\")\n",
        "dataloader_dev = create_dataloader(features_dev, dev=True)\n",
        "del features_dev"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAHA7ufR-yz_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(f\"/content/drive/My Drive/Cambridge/L101/{model_fname}\", num_labels=2)\n",
        "model.cuda()\n",
        "pass # suppress model.cuda output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIv2h3aDYrlq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unpack_batch(batch):\n",
        "    input_ids = batch[0]\n",
        "    attention_mask = batch[1]\n",
        "    type_ids = batch[2]\n",
        "    y_true = batch[3]\n",
        "    claim_id = batch[4]\n",
        "    doc_id = [doc_ids[idx] for idx in batch[5]]  #retrieve doc ids\n",
        "    sentence_idx = [sentence_idxs[idx] for idx in batch[5]]  #retrieve sentence_idx\n",
        "    return input_ids, attention_mask, type_ids, y_true, claim_id, doc_id, sentence_idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn9NJUTnejTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evidence = dict((el,[]) for el in dict.fromkeys(claim_ids))\n",
        "evidence_all_scores = dict((el,[]) for el in dict.fromkeys(claim_ids))\n",
        "print(\"Done\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08esE86bAISW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.eval()\n",
        "\n",
        "print(\"Start evaluation\")\n",
        "\n",
        "# Variables for evaluation\n",
        "eval_loss, eval_accuracy = 0, 0\n",
        "nb_eval_steps, nb_eval_examples = 0, 0\n",
        "conf_matrix = np.zeros((2,2))\n",
        "\n",
        "print(f\"Number of batches: {len(dataloader_dev)}\")\n",
        "for step, batch in enumerate(dataloader_dev):\n",
        "    if step % 1000 == 0:\n",
        "            print(f'\\nAt step {step}')\n",
        "    # Move batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack values\n",
        "    input_ids, attention_mask, type_ids, y_true, claim_ids_batch, doc_ids_batch, sentence_idxs_batch = unpack_batch(batch)\n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up validation. taken from https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "        outputs = model(input_ids, token_type_ids=type_ids, attention_mask=attention_mask, labels=y_true)\n",
        "        logits = outputs[1]\n",
        "\n",
        "\n",
        "    # Move logits and labels to CPU. from https://mccormickml.com/2019/07/22/BERT-fine-tuning/, as this should free up RAM\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    y_true = y_true.to('cpu').numpy()\n",
        "    pred_flat = np.argmax(logits, axis=1).flatten()\n",
        "    y_true_flat = y_true.flatten()\n",
        "\n",
        "    for i, claim_id in enumerate(claim_ids_batch):\n",
        "        softmax_logits = softmax(logits[i]) # !!!\n",
        "        # print(softmax_logits)\n",
        "        # print(f'For claim {claim_id}; doc_id: {doc_ids_batch[i]}; sentence idx: {sentence_idxs_batch[i]}')\n",
        "        # save all regression scores to dict\n",
        "        evidence_all_scores[claim_id.item()].append([softmax_logits[1], doc_ids_batch[i], sentence_idxs_batch[i]])\n",
        "        \n",
        "        # save just the sentences labeled as evidence to dict\n",
        "        if pred_flat[i] == 1: # only if classified as evidence\n",
        "            evidence[claim_id.item()].append([softmax_logits[1], doc_ids_batch[i], sentence_idxs_batch[i]])    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqqSRkppbLkG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "with open(f'/content/drive/My Drive/Overig/sentence_evidence_from{features_fname}.pkl', 'wb') as f:\n",
        "    pickle.dump(evidence, f)\n",
        "with open(f'/content/drive/My Drive/Overig/sentence_evidence_all_from{features_fname}.pkl', 'wb') as f:\n",
        "    pickle.dump(evidence_all_scores, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uSiFDg0FT83",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}