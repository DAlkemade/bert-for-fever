{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L101_evaluate_sentence_predictions",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DAlkemade/bert-for-fever/blob/master/L101_evaluate_sentence_predictions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8WRYYbj14bE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "import copy\n",
        "import pprint\n",
        "import random\n",
        "import sqlite3\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaeKUeh35SXA",
        "colab_type": "code",
        "outputId": "c69e0c1c-8007-4a1f-8611-1ebcf70d055c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P66M-aSf_-y4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "docs_input_file = '/content/drive/My Drive/Overig/dev_complete.sentences_id=k5t.p5.s5.jsonl'\n",
        "N = 5\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "PERMUTATION = False\n",
        "WORK_DIR = '/content/drive/My Drive/Overig/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMRbMoPT4FLe",
        "colab_type": "code",
        "outputId": "342ab66a-82b1-4555-a76e-8c32246e477c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with open(docs_input_file, \"r\") as in_file:\n",
        "    instances_input = []\n",
        "    for line in in_file:\n",
        "        instances_input.append(json.loads(line))\n",
        "print(len(instances_input))\n",
        "baseline_prediction = copy.deepcopy(instances_input)\n",
        "\n",
        "for instance in instances_input:\n",
        "    instance.pop('predicted_sentences', None) # drop all predicted sentences, since that's what we're doing"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqqSRkppbLkG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/drive/My Drive/Overig/sentence_evidence_from191219145714features_include_title=True_from_dev_from_baseline_docs_complete_fixed.pkl', 'rb') as f:\n",
        "    evidence_titles = pickle.load(f)\n",
        "with open('/content/drive/My Drive/Overig/sentence_evidence_all_from191219145714features_include_title=True_from_dev_from_baseline_docs_complete_fixed.pkl', 'rb') as f:\n",
        "    evidence_titles_all_scores = pickle.load(f)\n",
        "with open('/content/drive/My Drive/Overig/sentence_evidence_fromfeatures_dev_from_baseline_docs_complete_fixed.pkl', 'rb') as f:\n",
        "    evidence_no_titles = pickle.load(f)\n",
        "with open('/content/drive/My Drive/Overig/sentence_evidence_all_fromfeatures_dev_from_baseline_docs_complete_fixed.pkl', 'rb') as f:\n",
        "    evidence_no_titles_all_scores = pickle.load(f)\n",
        "with open('/content/drive/My Drive/Overig/sentence_evidence_from200110134032features_include_title=False_from_dev_sentences_from_bert_doc_selector.pkl', 'rb') as f:\n",
        "    evidence_bert = pickle.load(f)\n",
        "with open('/content/drive/My Drive/Overig/sentence_evidence_all_from200110134032features_include_title=False_from_dev_sentences_from_bert_doc_selector.pkl', 'rb') as f:\n",
        "    evidence_all_bert = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlLOLtM2cyti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mean_number_evidence_predictions_per_claim = np.mean([len(value) for value in evidence.values()])\n",
        "# mean_number_evidence_predictions_per_claim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GUROMt0fDli",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import heapq\n",
        "\n",
        "def save_similar(evidence, n_max=N, save=True):\n",
        "    instances = copy.deepcopy(instances_input)\n",
        "    for instance in instances:\n",
        "        id = instance['id']\n",
        "        if id in evidence.keys(): # so if verifiable\n",
        "            predicted_sentences = evidence[id]\n",
        "            # print(predicted_sentences)\n",
        "            predicted_sentences_sorted = sorted(predicted_sentences, key=lambda tup: tup[0])\n",
        "            # print(predicted_sentences_sorted)\n",
        "\n",
        "            n= min(n_max,len(predicted_sentences_sorted))\n",
        "            prediction = [[l[1], l[2]] for l in predicted_sentences_sorted[-n:]]\n",
        "        else: # so if not verifiable\n",
        "            prediction = [] # maybe random article instead?\n",
        "        instance['predicted_sentences'] = prediction\n",
        "    \n",
        "    if save:\n",
        "        with open(os.path.join(WORK_DIR, f'bert_predictions_sentences.jsonl'), \"w+\") as out_file:\n",
        "            for instance in instances:\n",
        "                out_file.write(json.dumps(instance) + \"\\n\")\n",
        "        print(f\"Saved file\")  \n",
        "    return instances          \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsRpZgamHNZA",
        "colab_type": "code",
        "outputId": "caeab972-c920-436e-b55e-70e643894a0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predictions_titles_high_precision = save_similar(evidence_titles, save=False)\n",
        "predictions_titles_high_recall = save_similar(evidence_titles_all_scores, save=False)\n",
        "predictions_no_titles_high_precision = save_similar(evidence_no_titles, save=False)\n",
        "predictions_no_titles_high_recall = save_similar(evidence_no_titles_all_scores, save=False)\n",
        "predictions_bert_docs = save_similar(evidence_bert, save=True)\n",
        "predictions_all_bert_docs = save_similar(evidence_all_bert, save=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vE5Tx6oCPnsh",
        "colab": {}
      },
      "source": [
        "def evaluate(instances_tmp):\n",
        "    results = []\n",
        "    prediction_lengths = []\n",
        "    \n",
        "    avoidable_mistakes = []\n",
        "    for instance in instances_tmp:\n",
        "        id = instance['id']\n",
        "        if instance['verifiable'] == 'VERIFIABLE': # so if verifiable\n",
        "\n",
        "            predicted = instance['predicted_sentences']\n",
        "            predicted_pages = instance['predicted_pages']\n",
        "            prediction_lengths.append(len(predicted))\n",
        "            # print(all_evi)\n",
        "            # print(f'{predicted}\\n')\n",
        "            found = 0\n",
        "            for evidence_group in instance[\"evidence\"]:\n",
        "                evidence = [[e[2], e[3]] for e in evidence_group]\n",
        "                if all([item in predicted for item in evidence]):\n",
        "                    # We only want to score complete groups of evidence. Incomplete groups are worthless.\n",
        "                    found = 1\n",
        "\n",
        "            if found == 0:\n",
        "                for evidence_group in instance[\"evidence\"]:\n",
        "                    evidence = [[e[2], e[3]] for e in evidence_group]\n",
        "                    for item in evidence:\n",
        "                        if not item in predicted and item[0] in predicted_pages:\n",
        "                            avoidable_mistakes.append([id, instance['claim'], item[0], item[1], instance['label'], predicted])\n",
        "\n",
        "               \n",
        "                            \n",
        "            results.append(found)\n",
        "                \n",
        "    print(f'Fully supported fraction {np.mean(results)}')\n",
        "    print(f'Number of checked claims: {len(results)}')\n",
        "    print(f'Average prediction length: {np.mean(prediction_lengths)}')\n",
        "    return results, avoidable_mistakes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogs5f8SuCnXP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "recall_arrays = {}\n",
        "print(\"Baseline\")\n",
        "recall_arrays['baseline'], errors = evaluate(baseline_prediction)\n",
        "print(\"Titles 1\")\n",
        "recall_arrays['titles+force5'], _ = evaluate(predictions_titles_high_recall)\n",
        "print(\"Titles 2\")\n",
        "recall_arrays['titles+noforce'], _ = evaluate(predictions_titles_high_precision)\n",
        "print(\"No titles 1\")\n",
        "recall_arrays['no_titles+force5'], _ = evaluate(predictions_no_titles_high_recall)\n",
        "print(\"No titles 2\")\n",
        "recall_arrays['no_titles+noforce'], _ = evaluate(predictions_no_titles_high_precision)\n",
        "print(\"Bert\")\n",
        "recall_arrays['bertdocs'], _ = evaluate(predictions_bert_docs)\n",
        "print(\"Bert all\")\n",
        "recall_arrays['bertdocs_all'], _ = evaluate(predictions_all_bert_docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAnom2ZUG5xP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from L90 project\n",
        "R = 5000  # Number of permutations\n",
        "\n",
        "\n",
        "def single_permute(results_a, results_b):\n",
        "    \"\"\"\n",
        "    Perform a single permutation test on two result vectors.\n",
        "    \"\"\"\n",
        "    vector_length = len(results_a)\n",
        "    swap_vector = np.random.randint(0, 2, vector_length)\n",
        "    permuted_a = results_a.copy()\n",
        "    permuted_b = results_b.copy()\n",
        "    for i in range(vector_length):\n",
        "        if swap_vector[i] == 1:\n",
        "            permuted_a[i] = results_b[i]\n",
        "            permuted_b[i] = results_a[i]\n",
        "    original_difference = abs(np.mean(results_a) - np.mean(results_b))\n",
        "    permuted_difference = abs(np.mean(permuted_a) - np.mean(permuted_b))\n",
        "    return permuted_difference >= original_difference\n",
        "\n",
        "\n",
        "def permutation_test(results_a, results_b):\n",
        "    \"\"\"\n",
        "    Perform a single permutation test on two result vectors. It returns the probability that the null hypothesis is true\n",
        "    \"\"\"\n",
        "    s = 0\n",
        "    for i in range(R):\n",
        "        if single_permute(results_a, results_b):\n",
        "            s += 1\n",
        "    p = (s + 1) / (R + 1)\n",
        "    return p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfGep9V0RiwZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if PERMUTATION:\n",
        "    permutation_scores = {}\n",
        "    for model_name in recall_arrays.keys():\n",
        "        permutation_scores[model_name] = {}\n",
        "\n",
        "    for model_name1, pred1 in recall_arrays.items():\n",
        "        for model_name2, pred2 in recall_arrays.items():\n",
        "            permutation_scores[model_name1][model_name2] = permutation_test(np.array(pred1), np.array(pred2))\n",
        "\n",
        "    pp.pprint(permutation_scores)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXkXqQrTcjgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_doc_text(id):\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(\n",
        "        \"SELECT lines FROM documents WHERE id = ?\",\n",
        "        (id,)\n",
        "    )\n",
        "    result = cursor.fetchone()\n",
        "    cursor.close()\n",
        "    return result\n",
        "\n",
        "def parse_doc(doc_raw):\n",
        "    \"\"\"\n",
        "    Parse a list of lines from a raw document text, with the index in the list\n",
        "    correponding to the line index in the data entries\n",
        "    \"\"\"\n",
        "    new = []\n",
        "    lines = doc_raw.split(\"\\n\")\n",
        "    char_count = 0\n",
        "    for line in lines:\n",
        "        # print('Line: {}'.format(line))\n",
        "        line = line.split(\"\\t\")\n",
        "        \n",
        "    #   TODO: THIS MIGHT DROP PARTS OF SENTENCES AFTER A TAB\n",
        "        if len(line[1]) > 1:\n",
        "            new.append(line[1])\n",
        "            char_count += len(line[1])\n",
        "        else:\n",
        "            new.append(EMPTY_TOKEN)\n",
        "    chars.append(char_count)\n",
        "    return new"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EuCN0j_dPeu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fever_db = 'fever.db'\n",
        "root = '/content/drive/My Drive/Overig/'\n",
        "\n",
        "db = os.path.join(root, fever_db)\n",
        "conn = sqlite3.connect(db)\n",
        "EMPTY_TOKEN = 'E'\n",
        "chars = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsfeykNxp26y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_sentence(doc, idx):\n",
        "    # print(f\"Get doc {doc} index {idx}\")\n",
        "    doc_raw = get_doc_text(doc)[0]\n",
        "    # print(doc_raw)\n",
        "    parsed_doc = parse_doc(doc_raw)\n",
        "    sentence = parsed_doc[idx]\n",
        "    return sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3reK5FaZk4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random.Random(4).shuffle(errors)\n",
        "selected_errors = errors[:100]\n",
        "for e in selected_errors:\n",
        "    sentence = get_sentence(e[2], e[3])\n",
        "    e.append(sentence)\n",
        "    predictions = e[5]\n",
        "    e.remove(predictions)\n",
        "    for i in range(5):\n",
        "        if i < len(predictions):\n",
        "            e.append(get_sentence(predictions[i][0], predictions[i][1]))\n",
        "        else:\n",
        "            e.append(None)\n",
        "\n",
        "\n",
        "# pp.pprint(selected_errors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty_14rlGfBMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "selected_errors_pd = pd.DataFrame(selected_errors, columns=['claim_id', 'claim_text', 'doc_id', 'sentence_index', 'label', 'sentence_text', 'wrong1', 'wrong2', 'wrong3', 'wrong4', 'wrong5'])\n",
        "selected_errors_pd.to_csv('/content/drive/My Drive/Overig/error_analysis.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5exke_rzlEsn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for e in errors:\n",
        "    label = 1 if e[4] == 'SUPPORTS' else 0\n",
        "    e.append(label)\n",
        "errors_pd = pd.DataFrame(errors, columns=['claim_id', 'claim_text', 'doc_id', 'sentence_index', 'label', 'label_int'])\n",
        "np.mean(errors_pd.label_int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8NfD3QEXsSX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uSiFDg0FT83",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}