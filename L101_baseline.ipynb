{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L101_baseline",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DAlkemade/bert-for-fever/blob/master/L101_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_58nCvwEE5J",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kLmJ9p_EfP5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install -r \"/content/drive/My Drive/Overig/requirements.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePXczKS9HlPn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egXzhWthHbga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "WORK_DIR = '/content/drive/My Drive/Overig'\n",
        "db_file = os.path.join(WORK_DIR, 'fever.db')\n",
        "index_file = os.path.join(WORK_DIR, 'fever-tfidf-ngram=2-hash=16777216-tokenizer=simple.npz')\n",
        "in_file = os.path.join(WORK_DIR, 'test.jsonl')\n",
        "out_file = os.path.join(WORK_DIR, 'test_baseline_pages.sentences.p5.s5.jsonl')\n",
        "EMPTY_TOKEN = 'EMPTY'\n",
        "TESTING = False\n",
        "PREDICT_SENTENCES = False\n",
        "MAX_DOCS = 50\n",
        "MAX_SENTS = 5\n",
        "print(db_file)\n",
        "print(index_file)\n",
        "print(in_file)\n",
        "print(out_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBDI4JZEEXq4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from drqa.retriever import DocDB, utils\n",
        "\n",
        "\n",
        "# Copied FeverDocDB from Papelo repo\n",
        "class FEVERDocumentDatabase(DocDB):\n",
        "\n",
        "    def __init__(self,path=None):\n",
        "        super().__init__(path)\n",
        "\n",
        "    def get_doc_lines(self, doc_id):\n",
        "        \"\"\"Fetch the raw text of the doc for 'doc_id'.\"\"\"\n",
        "        cursor = self.connection.cursor()\n",
        "        cursor.execute(\n",
        "            \"SELECT lines FROM documents WHERE id = ?\",\n",
        "            (utils.normalize(doc_id),)\n",
        "        )\n",
        "        result = cursor.fetchone()\n",
        "        cursor.close()\n",
        "        return result if result is None else result[0]\n",
        "\n",
        "    def get_non_empty_doc_ids(self):\n",
        "        \"\"\"Fetch all ids of docs stored in the db.\"\"\"\n",
        "        cursor = self.connection.cursor()\n",
        "        cursor.execute(\"SELECT id FROM documents WHERE length(trim(text)) > 0\")\n",
        "        results = [r[0] for r in cursor.fetchall()]\n",
        "        cursor.close()\n",
        "        return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVnUQ7rIESHs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.common import Registrable\n",
        "from fever.reader.document_database import FEVERDocumentDatabase\n",
        "\n",
        "\n",
        "class RetrievalMethod(Registrable):\n",
        "\n",
        "    def __init__(self,database:FEVERDocumentDatabase):\n",
        "        self.database = database\n",
        "\n",
        "    def get_sentences_for_claim(self,claim_text,include_text=False):\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g01ePZ9zOBL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_doc(doc_raw):\n",
        "    \"\"\"\n",
        "    Parse a list of lines from a raw document text, with the index in the list\n",
        "    correponding to the line index in the data entries\n",
        "    \"\"\"\n",
        "    new = []\n",
        "    #   lines = doc_raw.split(\"\\n\")\n",
        "    for line in doc_raw:\n",
        "        # print('Line: {}'.format(line))\n",
        "        line = line.split(\"\\t\")\n",
        "    #   TODO: THIS MIGHT DROP PARTS OF SENTENCES AFTER A TAB\n",
        "        if len(line[1]) > 1:\n",
        "            new.append(line[1])\n",
        "        else:\n",
        "            new.append(EMPTY_TOKEN)\n",
        "    return new"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BZBJqATEH-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from drqa import retriever\n",
        "from drqascripts.retriever.build_tfidf_lines import OnlineTfidfDocRanker\n",
        "\n",
        "@RetrievalMethod.register(\"top_docs\")\n",
        "class TopNDocsTopNSents(RetrievalMethod):\n",
        "\n",
        "    class RankArgs:\n",
        "        def __init__(self):\n",
        "            self.ngram = 2\n",
        "            self.hash_size = int(math.pow(2,24))\n",
        "            self.tokenizer = \"simple\"\n",
        "            self.num_workers = None\n",
        "\n",
        "    def __init__(self, database, index, n_docs, n_sents):\n",
        "        super().__init__(database)\n",
        "        self.n_docs = n_docs\n",
        "        self.n_sents = n_sents\n",
        "        print(\"Retrieve tfidf indices\")\n",
        "        self.ranker = retriever.get_class('tfidf')(tfidf_path=index)\n",
        "        print(\"Retrieved tfidf indices\")\n",
        "        self.onlineranker_args = self.RankArgs()\n",
        "\n",
        "    def get_docs_for_claim(self, claim_text):\n",
        "        doc_names, doc_scores = self.ranker.closest_docs(claim_text, self.n_docs)\n",
        "        return zip(doc_names, doc_scores)\n",
        "\n",
        "    def tf_idf_sim(self, claim, lines, freqs=None):\n",
        "        tfidf = OnlineTfidfDocRanker(self.onlineranker_args, [line[\"sentence\"] for line in lines], freqs)\n",
        "        line_ids, scores = tfidf.closest_docs(claim,self.n_sents)\n",
        "        ret_lines = []\n",
        "        for idx, line in enumerate(line_ids):\n",
        "            ret_lines.append(lines[line])\n",
        "            ret_lines[-1][\"score\"] = scores[idx]\n",
        "        return ret_lines\n",
        "    \n",
        "    def get_only_docs_for_claim(self, claim_text):\n",
        "        pages = self.get_docs_for_claim(claim_text)\n",
        "        sorted_p = list(sorted(pages, reverse=True, key=lambda elem: elem[1]))\n",
        "        pages = [p[0] for p in sorted_p[:self.n_docs]]\n",
        "        return pages    \n",
        "\n",
        "    def get_sentences_for_claim(self,claim_text,include_text=False):\n",
        "        pages = self.get_docs_for_claim(claim_text)\n",
        "        sorted_p = list(sorted(pages, reverse=True, key=lambda elem: elem[1]))\n",
        "        pages = [p[0] for p in sorted_p[:self.n_docs]]\n",
        "        p_lines = []\n",
        "        for page in pages:\n",
        "            lines = self.database.get_doc_lines(page)\n",
        "            lines = parse_doc(lines)\n",
        "\n",
        "            p_lines.extend(zip(lines, [page] * len(lines), range(len(lines))))\n",
        "        lines = []\n",
        "        for p_line in p_lines:\n",
        "            lines.append({\n",
        "                \"sentence\": p_line[0],\n",
        "                \"page\": p_line[1],\n",
        "                \"line_on_page\": p_line[2]\n",
        "            })\n",
        "        scores = self.tf_idf_sim(claim_text, lines)\n",
        "\n",
        "        if include_text:\n",
        "            return scores\n",
        "\n",
        "        return [(s[\"page\"], s[\"line_on_page\"]) for s in scores]\n",
        "\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IR7G3dbCmfF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import json\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from tqdm import tqdm\n",
        "\n",
        "import multiprocessing\n",
        "\n",
        "CORES = multiprocessing.cpu_count()\n",
        "\n",
        "\n",
        "def process_line(method,line, args):\n",
        "    if PREDICT_SENTENCES:\n",
        "        sents = method.get_sentences_for_claim(line[\"claim\"])\n",
        "        pages = list(set(map(lambda sent:sent[0],sents)))\n",
        "        line[\"predicted_pages\"] = pages\n",
        "        line[\"predicted_sentences\"] = sents\n",
        "        return line\n",
        "    else:\n",
        "        pages = list(method.get_only_docs_for_claim(line[\"claim\"]))\n",
        "        line[\"predicted_pages\"] = pages\n",
        "        return line\n",
        "\n",
        "\n",
        "def str2bool(v):\n",
        "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
        "        return True\n",
        "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
        "        return False\n",
        "    else:\n",
        "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
        "\n",
        "\n",
        "def get_map_function(parallel):\n",
        "    return p.imap if parallel else map\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--database', type=str, help='/path/to/saved/db.db')\n",
        "    parser.add_argument('--index', type=str, help='/path/to/saved/db.db')\n",
        "    parser.add_argument('--in-file', type=str, help='/path/to/saved/db.db')\n",
        "    parser.add_argument('--out-file', type=str, help='/path/to/saved/db.db')\n",
        "    parser.add_argument('--max-page',type=int)\n",
        "    parser.add_argument('--max-sent',type=int)\n",
        "    parser.add_argument('--cuda-device',type=int,default=-1)\n",
        "    parser.add_argument('--parallel',type=str2bool,default=True)\n",
        "    parser.add_argument('--threads', type=int, default=None)\n",
        "    sequence = f'--database /content/drive/My Drive/Overig/fever.db --index /content/drive/My Drive/Overig/fever-tfidf-ngram=2-hash=16777216-tokenizer=simple.npz --in-file /content/drive/My Drive/Overig/dev.jsonl --out-file /content/drive/My Drive/Overig/dev.sentences.p5.s5.jsonl --max-page 5 --max-sent 5'\n",
        "    sequence_list = ['--database', db_file, '--index', index_file, '--in-file', in_file, '--out-file', out_file, '--max-page', str(MAX_DOCS), '--max-sent', str(MAX_SENTS)]\n",
        "    print(sequence_list)\n",
        "    args = parser.parse_args(sequence_list) \n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAqbbMd8VVuX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Create database object')\n",
        "database = FEVERDocumentDatabase(args.database)\n",
        "print(\"Create TopNDocsTopNSents object\")\n",
        "method = TopNDocsTopNSents(database, args.index, args.max_page, args.max_sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Z04gDbpAmE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "old_preds = []\n",
        "completed_claim_ids = []\n",
        "\n",
        "# with open('/content/drive/My Drive/Overig/train_baseline_pages.sentences.p5.s5.jsonl',\"r\") as prev_prediction_file:\n",
        "\n",
        "#     for line in prev_prediction_file:\n",
        "#         old_preds.append(json.loads(line))\n",
        "#     \n",
        "#     for pred in old_preds:\n",
        "#         completed_claim_ids.append(pred['id'])\n",
        "\n",
        "len(completed_claim_ids)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "757BL4yCvPUZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Using {} cores\".format(args.threads))\n",
        "print(\"Start processing\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "il0WvFwoOv-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processed = dict()\n",
        "with open(args.in_file,\"r\") as in_file, open(args.out_file, \"w+\") as out_file:\n",
        "    lines = []\n",
        "    for line in in_file:\n",
        "        lines.append(json.loads(line))\n",
        "    # print(f'Length OG set: {len(lines)}')\n",
        "    \n",
        "    if TESTING:\n",
        "        lines = lines[:10]\n",
        "    uncompleted_lines = []\n",
        "    for line in lines:\n",
        "        if not line['id'] in completed_claim_ids:\n",
        "            uncompleted_lines.append(line)\n",
        "    for pred in old_preds:\n",
        "        out_file.write(json.dumps(pred) + \"\\n\")\n",
        "    # print(f'\\nLength uncompleted: {len(uncompleted_lines)}')\n",
        "\n",
        "    with ThreadPool(args.threads) as p:\n",
        "        for line in tqdm(get_map_function(args.parallel)(lambda line: process_line(method, line, args), uncompleted_lines),\n",
        "                            total=len(uncompleted_lines)):\n",
        "            out_file.write(json.dumps(line) + \"\\n\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}